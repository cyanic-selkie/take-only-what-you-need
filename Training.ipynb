{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8d7b463",
   "metadata": {},
   "outputs": [],
   "source": [
    "import simdjson\n",
    "import itertools\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from scipy import sparse\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import string\n",
    "import fasttext as ft\n",
    "import fasttext.util\n",
    "from torch import nn\n",
    "import torch\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "import pytorch_lightning as pl\n",
    "from torch.nn import functional as F\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "382b9e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/dataset_100.json\") as f:\n",
    "    dataset = simdjson.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fbf116f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = [], []\n",
    "\n",
    "for key, group in itertools.groupby(dataset, lambda x: x[\"document_id\"]):\n",
    "    X.append(key)\n",
    "    Y.append(next(group)[\"labels\"])\n",
    "\n",
    "mlb = MultiLabelBinarizer().fit(Y)\n",
    "X = np.array(X)\n",
    "Y = mlb.transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53033d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = mlb.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7246506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "642\n"
     ]
    }
   ],
   "source": [
    "msss = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=0.3)\n",
    "\n",
    "for train_index, test_index in msss.split(X, Y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "    \n",
    "    break\n",
    "    \n",
    "useless = set(itertools.chain(*mlb.inverse_transform(Y_test))) ^ set(itertools.chain(*mlb.inverse_transform(Y_train)))\n",
    "\n",
    "print(len(useless))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "862bc209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#weights = (Y_train.shape[0] - np.sum(Y_train, axis=0)) / np.sum(Y_train, axis=0)\n",
    "\n",
    "\n",
    "np.sum(Y_train, axis=0) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b12e7b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_stats(p):\n",
    "    sentence_lengths = []\n",
    "    sentence_counts = []\n",
    "    sentence_ratios = []\n",
    "    sentence_counts_original = []\n",
    "\n",
    "    for key, group in itertools.groupby(dataset, lambda x: x[\"document_id\"]):\n",
    "        group = list(group)\n",
    "        \n",
    "        sentence_counts_original.append(len(group))\n",
    "        group = group[:max(ceil(p * len(group)), 1)]\n",
    "        sentence_counts.append(len(group))\n",
    "        for sentence in group:\n",
    "            sentence_lengths.append(len(sentence[\"lemmas\"]))\n",
    "\n",
    "            junk_count = [i for i in sentence[\"lemmas\"] if all(j.isdigit() or j in string.punctuation for j in i)]\n",
    "            sentence_ratio = len(junk_count) / len(sentence[\"lemmas\"])\n",
    "            sentence_ratios.append(sentence_ratio)\n",
    "\n",
    "    sentence_lengths = np.array(sentence_lengths)\n",
    "    sentence_counts = np.array(sentence_counts)\n",
    "    sentence_ratios = np.array(sentence_ratios)\n",
    "    sentence_counts_original = np.array(sentence_counts_original)\n",
    "\n",
    "    return sentence_lengths, sentence_counts, sentence_ratios, sentence_counts_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72872453",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sentence_stats(sentence_lengths, sentence_counts, sentence_ratios, sentence_counts_original):\n",
    "    sentence_removed_ratio = 1 - sentence_counts / sentence_counts_original\n",
    "    \n",
    "    quantile = np.quantile(sentence_lengths, 0.9)\n",
    "    sentence_lengths = sentence_lengths[sentence_lengths < quantile]\n",
    "\n",
    "    quantile = np.quantile(sentence_counts, 0.9)\n",
    "    sentence_counts = sentence_counts[sentence_counts < quantile]\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=2, ncols=2)\n",
    "\n",
    "    fig.set_size_inches(20, 15)\n",
    "\n",
    "    ax[0, 0].bar(*np.unique(sentence_lengths, return_counts=True))\n",
    "    ax[0, 1].bar(*np.unique(sentence_counts[sentence_counts < quantile], return_counts=True))\n",
    "    ax[1, 0].hist(sentence_ratios, bins=100)\n",
    "    ax[1, 1].hist(sentence_removed_ratio, bins=100)\n",
    "\n",
    "    ax[0, 0].set_ylabel('Count')\n",
    "    ax[0, 0].set_xlabel('Sentence length');\n",
    "\n",
    "    ax[0, 1].set_ylabel('Count')\n",
    "    ax[0, 1].set_xlabel('Sentence count');\n",
    "\n",
    "    ax[1, 0].set_ylabel('Count')\n",
    "    ax[1, 0].set_xlabel('Waste ratio');\n",
    "    \n",
    "    ax[1, 1].set_ylabel('Count')\n",
    "    ax[1, 1].set_xlabel('Ratio of removed sentences');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308553e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_lengths_25, sentence_counts_25, sentence_ratios_25, sentence_counts_original_25 = sentence_stats(0.25)\n",
    "plot_sentence_stats(sentence_lengths_25, sentence_counts_25, sentence_ratios_25, sentence_counts_original_25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6298b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_lengths_50, sentence_counts_50, sentence_ratios_50, sentence_counts_original_50 = sentence_stats(0.50)\n",
    "plot_sentence_stats(sentence_lengths_50, sentence_counts_50, sentence_ratios_50, sentence_counts_original_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1600f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_lengths_75, sentence_counts_75, sentence_ratios_75, sentence_counts_original_75 = sentence_stats(0.75)\n",
    "plot_sentence_stats(sentence_lengths_75, sentence_counts_75, sentence_ratios_75, sentence_counts_original_75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff3cd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_lengths_100, sentence_counts_100, sentence_ratios_100, sentence_counts_original_100 = sentence_stats(1)\n",
    "plot_sentence_stats(sentence_lengths_100, sentence_counts_100, sentence_ratios_100, sentence_counts_original_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cbf2804",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "ft_model = ft.load_model('cc.hr.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fce148b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(p):\n",
    "    embeddings = {}\n",
    "\n",
    "    for key, group in itertools.groupby(dataset, lambda x: x[\"document_id\"]):\n",
    "        group = list(group)\n",
    "        group = group[:max(ceil(p * len(group)), 1)]\n",
    "        \n",
    "        embeddings[key] = [ft_model.get_sentence_vector(\" \".join(sentence[\"tokens\"])) for sentence in group]\n",
    "        \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecce2f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EuroVocDataset(Dataset):\n",
    "    def __init__(self, embeddings, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.embeddings = embeddings\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        ids = self.X[idx]\n",
    "        labels = self.Y[idx]\n",
    "        \n",
    "        return {\"input\": self.embeddings[ids], \"output\": labels.astype(np.float32)}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "def collate(datapoints):\n",
    "    max_len = max([len(datapoint[\"input\"]) for datapoint in datapoints])\n",
    "    \n",
    "    X = []\n",
    "    Y = []\n",
    "    for datapoint in datapoints:\n",
    "        missing = max_len - len(datapoint[\"input\"])\n",
    "        \n",
    "        if missing > 0:\n",
    "            X.append(np.append(datapoint[\"input\"], np.zeros((missing, 300)), axis=0).astype(np.float32))\n",
    "        else:\n",
    "            X.append(np.array(datapoint[\"input\"]).astype(np.float32))\n",
    "\n",
    "        Y.append(datapoint[\"output\"])\n",
    "                \n",
    "    return {\"input\": torch.from_numpy(np.stack(X)), \"output\": torch.from_numpy(np.stack(Y))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c57c621",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = generate_embeddings(0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5069718",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = EuroVocDataset(embeddings, X_train, Y_train)\n",
    "dataset_val = EuroVocDataset(embeddings, X_test, Y_test)\n",
    "\n",
    "train_loader = DataLoader(dataset_train, batch_size=16, shuffle=True, collate_fn=collate, num_workers=30)\n",
    "val_loader = DataLoader(dataset_val, batch_size=16, collate_fn=collate, num_workers=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1baf8d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=300, nhead=5)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
    "        self.output = nn.Linear(300, len(classes))\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = torch.max(x, 1)[0]\n",
    "        x = self.output(x)\n",
    "        \n",
    "        return x.float()\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=5e-4)\n",
    "        \n",
    "        return optimizer\n",
    "    \n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        x, y = train_batch[\"input\"], train_batch[\"output\"]\n",
    "        \n",
    "        y_pred = self.forward(x)\n",
    "        \n",
    "        loss = self.criterion(y_pred, y)\n",
    "        \n",
    "        self.log(\"train_loss\", loss)\n",
    "        \n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"expected\": y,\n",
    "            \"predicted\": y_pred\n",
    "        }\n",
    "        \n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        x, y = val_batch[\"input\"], val_batch[\"output\"]\n",
    "        \n",
    "        y_pred = self.forward(x)\n",
    "        \n",
    "        loss = self.criterion(y_pred, y)\n",
    "        \n",
    "        self.log(\"val_loss\", loss)\n",
    "        \n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"expected\": y,\n",
    "            \"predicted\": y_pred\n",
    "\n",
    "        }\n",
    "    \n",
    "    def training_epoch_end(self, outputs):\n",
    "        avg_loss = torch.FloatTensor([x[\"loss\"] for x in outputs]).mean()\n",
    "        \n",
    "        y_true = np.rint(torch.cat([x[\"expected\"] for x in outputs]).cpu())\n",
    "        y_pred = np.rint(torch.sigmoid(torch.cat([x[\"predicted\"] for x in outputs])).detach().cpu())\n",
    "        \n",
    "        r_macro = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "        r_micro = recall_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "        \n",
    "        p_macro = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "        p_micro = precision_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "        \n",
    "        f1_macro = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "        f1_micro = f1_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "\n",
    "        self.logger.experiment.add_scalar(\"Loss/Train\", avg_loss, self.current_epoch)\n",
    "        self.logger.experiment.add_scalar(\"Recall/Macro/Train\", r_macro, self.current_epoch)\n",
    "        self.logger.experiment.add_scalar(\"Recall/Micro/Train\", r_micro, self.current_epoch)\n",
    "        self.logger.experiment.add_scalar(\"Precision/Macro/Train\", p_macro, self.current_epoch)\n",
    "        self.logger.experiment.add_scalar(\"Precision/Micro/Train\", p_macro, self.current_epoch)\n",
    "        self.logger.experiment.add_scalar(\"F1/Macro/Train\", f1_macro, self.current_epoch)\n",
    "        self.logger.experiment.add_scalar(\"F1/Micro/Train\", f1_micro, self.current_epoch)\n",
    "        \n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.FloatTensor([x[\"loss\"] for x in outputs]).mean()\n",
    "        \n",
    "        y_true = np.rint(torch.cat([x[\"expected\"] for x in outputs]).cpu())\n",
    "        y_pred = np.rint(torch.sigmoid(torch.cat([x[\"predicted\"] for x in outputs])).detach().cpu())\n",
    "        \n",
    "        r_macro = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "        r_micro = recall_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "        \n",
    "        p_macro = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "        p_micro = precision_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "        \n",
    "        f1_macro = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "        f1_micro = f1_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "\n",
    "        self.logger.experiment.add_scalar(\"Loss/Valid\", avg_loss, self.current_epoch)\n",
    "        self.logger.experiment.add_scalar(\"Recall/Macro/Valid\", r_macro, self.current_epoch)\n",
    "        self.logger.experiment.add_scalar(\"Recall/Micro/Valid\", r_micro, self.current_epoch)\n",
    "        self.logger.experiment.add_scalar(\"Precision/Macro/Valid\", p_macro, self.current_epoch)\n",
    "        self.logger.experiment.add_scalar(\"Precision/Micro/Valid\", p_micro, self.current_epoch)\n",
    "        self.logger.experiment.add_scalar(\"F1/Macro/Valid\", f1_macro, self.current_epoch)\n",
    "        self.logger.experiment.add_scalar(\"F1/Micro/Valid\", f1_micro, self.current_epoch)\n",
    "        \n",
    "        self.log(\"avg_val_loss\", avg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "44d27794",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b7cea6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: tb_logs/model_25\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | encoder   | TransformerEncoder | 9.6 M \n",
      "1 | output    | Linear             | 852 K \n",
      "2 | criterion | BCEWithLogitsLoss  | 0     \n",
      "-------------------------------------------------\n",
      "10.4 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.4 M    Total params\n",
      "41.655    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scurkovic/.conda/envs/research/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has\n",
      "                not been set for this class (_ResultMetric). The property determines if `update` by\n",
      "                default needs access to the full metric state. If this is not the case, significant speedups can be\n",
      "                achieved and we recommend setting this to `False`.\n",
      "                We provide an checking function\n",
      "                `from torchmetrics.utilities import check_forward_no_full_state`\n",
      "                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,\n",
      "                default for now) or if `full_state_update=False` can be used safely.\n",
      "                \n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/scurkovic/.conda/envs/research/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has\n",
      "                not been set for this class (_ResultMetric). The property determines if `update` by\n",
      "                default needs access to the full metric state. If this is not the case, significant speedups can be\n",
      "                achieved and we recommend setting this to `False`.\n",
      "                We provide an checking function\n",
      "                `from torchmetrics.utilities import check_forward_no_full_state`\n",
      "                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,\n",
      "                default for now) or if `full_state_update=False` can be used safely.\n",
      "                \n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73f3b5c231ee4e358935bad884741bce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logger = TensorBoardLogger(\"tb_logs\", name=\"model_25\")\n",
    "checkpointer = pl.callbacks.ModelCheckpoint(mode=\"min\", monitor=\"avg_val_loss\", dirpath=\"checkpoints\", filename=\"{epoch}-{avg_val_loss:.2f}\", save_on_train_epoch_end = True, every_n_epochs=1)\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=1000, devices=[0], strategy=\"dp\", logger=logger, accelerator=\"gpu\", auto_select_gpus=False, callbacks=[checkpointer])\n",
    "trainer.fit(clf, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9487bf93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
