{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0516219a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import simdjson\n",
    "import itertools\n",
    "from itertools import repeat\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
    "import string\n",
    "import fasttext as ft\n",
    "import fasttext.util as ft_util\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import multiprocessing\n",
    "import time\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "624c17b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/dataset_100.json\") as f:\n",
    "    dataset = simdjson.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b4ded526",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set([\"a\",\"ako\",\"ali\",\"bi\",\"bih\",\"bila\",\"bili\",\"bilo\",\"bio\",\"bismo\",\"biste\",\"biti\",\"bumo\",\"da\",\"do\",\"duž\",\"ga\",\"hoće\",\"hoćemo\",\"hoćete\",\"hoćeš\",\"hoću\",\"i\",\"iako\",\"ih\",\"ili\",\"iz\",\"ja\",\"je\",\"jedna\",\"jedne\",\"jedno\",\"jer\",\"jesam\",\"jesi\",\"jesmo\",\"jest\",\"jeste\",\"jesu\",\"jim\",\"joj\",\"još\",\"ju\",\"kada\",\"kako\",\"kao\",\"koja\",\"koje\",\"koji\",\"kojima\",\"koju\",\"kroz\",\"li\",\"me\",\"mene\",\"meni\",\"mi\",\"mimo\",\"moj\",\"moja\",\"moje\",\"mu\",\"na\",\"nad\",\"nakon\",\"nam\",\"nama\",\"nas\",\"naš\",\"naša\",\"naše\",\"našeg\",\"ne\",\"nego\",\"neka\",\"neki\",\"nekog\",\"neku\",\"nema\",\"netko\",\"neće\",\"nećemo\",\"nećete\",\"nećeš\",\"neću\",\"nešto\",\"ni\",\"nije\",\"nikoga\",\"nikoje\",\"nikoju\",\"nisam\",\"nisi\",\"nismo\",\"niste\",\"nisu\",\"njega\",\"njegov\",\"njegova\",\"njegovo\",\"njemu\",\"njezin\",\"njezina\",\"njezino\",\"njih\",\"njihov\",\"njihova\",\"njihovo\",\"njim\",\"njima\",\"njoj\",\"nju\",\"no\",\"o\",\"od\",\"odmah\",\"on\",\"ona\",\"oni\",\"ono\",\"ova\",\"pa\",\"pak\",\"po\",\"pod\",\"pored\",\"prije\",\"s\",\"sa\",\"sam\",\"samo\",\"se\",\"sebe\",\"sebi\",\"si\",\"smo\",\"ste\",\"su\",\"sve\",\"svi\",\"svog\",\"svoj\",\"svoja\",\"svoje\",\"svom\",\"ta\",\"tada\",\"taj\",\"tako\",\"te\",\"tebe\",\"tebi\",\"ti\",\"to\",\"toj\",\"tome\",\"tu\",\"tvoj\",\"tvoja\",\"tvoje\",\"u\",\"uz\",\"vam\",\"vama\",\"vas\",\"vaš\",\"vaša\",\"vaše\",\"već\",\"vi\",\"vrlo\",\"za\",\"zar\",\"će\",\"ćemo\",\"ćete\",\"ćeš\",\"ću\",\"što\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c67f0c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = [], []\n",
    "\n",
    "for key, group in itertools.groupby(dataset, lambda x: x[\"document_id\"]):\n",
    "    X.append(key)\n",
    "    Y.append(next(group)[\"labels\"])\n",
    "\n",
    "mlb = MultiLabelBinarizer().fit(Y)\n",
    "X = np.array(X)\n",
    "Y = mlb.transform(Y)\n",
    "\n",
    "classes = mlb.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42598d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "645\n"
     ]
    }
   ],
   "source": [
    "msss = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=0.3)\n",
    "\n",
    "for train_index, test_index in msss.split(X, Y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "    \n",
    "    break\n",
    "    \n",
    "useless = set(itertools.chain(*mlb.inverse_transform(Y_test))) ^ set(itertools.chain(*mlb.inverse_transform(Y_train)))\n",
    "\n",
    "print(len(useless))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8b60d3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(k, X_train, X_test):\n",
    "    documents = {}\n",
    "\n",
    "    for key, group in itertools.groupby(dataset, lambda x: x[\"document_id\"]):\n",
    "        group = list(group)\n",
    "        \n",
    "        sentences = []\n",
    "        for sentence in group:\n",
    "            if len(sentences) == k:\n",
    "                break\n",
    "                \n",
    "            filtered_indices = [i for i, lemma in enumerate(sentence[\"lemmas\"]) if not all(x.isdigit() or x in string.punctuation for x in lemma) and not lemma in stopwords]\n",
    "            filtered_tokens = [sentence[\"tokens\"][i] for i in filtered_indices]\n",
    "            \n",
    "            if len(filtered_tokens) < 5:\n",
    "                continue\n",
    "\n",
    "            sentences.append(\" \".join(filtered_tokens))\n",
    "\n",
    "        documents[key] = \" \".join(sentences)\n",
    "\n",
    "        if len(sentences) == 0:\n",
    "            documents[key] = \"UNK\"\n",
    "            \n",
    "    X_train_documents = []\n",
    "    X_test_documents = []\n",
    "    \n",
    "    for x in X_train:\n",
    "        X_train_documents.append(documents[x])\n",
    "    \n",
    "    for x in X_test:\n",
    "        X_test_documents.append(documents[x])\n",
    "        \n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X_train_features = vectorizer.fit_transform(X_train_documents)\n",
    "    X_test_features = vectorizer.transform(X_test_documents)\n",
    "    \n",
    "    return X_train_features, X_test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "24310826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7307, 43271)\n",
      "1.0 0.0\n",
      "1.01.0 0.0\n",
      "1.0 0.5\n",
      "1.0 0.5\n"
     ]
    }
   ],
   "source": [
    "X_train_features, X_test_features = generate_features(4, X_train, X_test)\n",
    "\n",
    "print(X_train_features.shape)\n",
    "\n",
    "for i in range(1, 5):\n",
    "    clf = LinearSVC(class_weight='balanced', max_iter=10000).fit(X_train_features, Y_train[:, i])\n",
    "    \n",
    "    f1_test = f1_score(Y_test[:, i], clf.predict(X_test_features), zero_division=0)\n",
    "    f1_train = f1_score(Y_train[:, i], clf.predict(X_train_features), zero_division=0)\n",
    "    print(f1_train, f1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "332aa83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(i, X_train_features, Y_train, X_test_features, Y_test):\n",
    "    params = {\n",
    "        \"C\" : [1]\n",
    "    }\n",
    "    \n",
    "    best_C = None\n",
    "    best_f1 = None\n",
    "    best_predictions = None\n",
    "    try:\n",
    "        for C in params[\"C\"]:\n",
    "            clf = LinearSVC(class_weight='balanced', max_iter=10000, C=C).fit(X_train_features, Y_train[:, i])\n",
    "            \n",
    "            Y_test_pred = clf.predict(X_test_features)\n",
    "            \n",
    "            f1 = f1_score(Y_test[:, i], Y_test_pred, zero_division=0)\n",
    "            \n",
    "            if best_C is None or best_f1 < f1:\n",
    "                best_f1 = f1\n",
    "                best_C = c\n",
    "                best_predictions = Y_test_pred\n",
    "    except ValueError:\n",
    "        best_predictions = np.zeros((Y_test.shape[0],))\n",
    "    \n",
    "    return best_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1f49a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-1303:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/scurkovic/.conda/envs/research/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/scurkovic/.conda/envs/research/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/scurkovic/.conda/envs/research/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/scurkovic/.conda/envs/research/lib/python3.8/multiprocessing/queues.py\", line 358, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "_pickle.UnpicklingError: invalid load key, '\\x00'.\n",
      "Process ForkPoolWorker-1371:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/scurkovic/.conda/envs/research/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/scurkovic/.conda/envs/research/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/scurkovic/.conda/envs/research/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/scurkovic/.conda/envs/research/lib/python3.8/multiprocessing/queues.py\", line 358, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "_pickle.UnpicklingError: invalid load key, '\\x00'.\n"
     ]
    }
   ],
   "source": [
    "pool = multiprocessing.Pool(10)\n",
    "\n",
    "scores = []\n",
    "for i in range(1, 16):\n",
    "    X_train_features, X_test_features = generate_features(i, X_train, X_test)\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    Y_test_pred = np.stack(pool.starmap(train_classifier, zip(range(Y_test.shape[1]), repeat(X_train_features), repeat(Y_train), repeat(X_test_features), repeat(Y_test))), axis=-1)\n",
    "    print(\"here\")\n",
    "    f1_macro = f1_score(Y_test, Y_test_pred, average='macro', zero_division=0)\n",
    "    f1_micro = f1_score(Y_test, Y_test_pred, average='micro', zero_division=0)\n",
    "\n",
    "    precision_macro = precision_score(Y_test, Y_test_pred, average='macro', zero_division=0)\n",
    "    precision_micro = precision_score(Y_test, Y_test_pred, average='micro', zero_division=0)\n",
    "\n",
    "    recall_macro = recall_score(Y_test, Y_test_pred, average='macro', zero_division=0)\n",
    "    recall_micro = recall_score(Y_test, Y_test_pred, average='micro', zero_division=0)\n",
    "\n",
    "    scores.append((f1_macro, f1_micro, precision_macro, precision_micro, recall_macro, recall_micro, time.perf_counter() - start))\n",
    "    \n",
    "    print(scores[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86bc0b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
